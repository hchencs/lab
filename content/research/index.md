+++
title = "Research"
page_template = "page.html"
+++

## AI-driven security and software engineering

Modern society depends on critical infrastructure run by large, complex software, but software is expensive to develop and difficult to verify. AI will change how we build, verify, and maintain software. Towards this goal, we are making progress on:

- Teaching AI models to understand and generate code ([EMNLP 2024](/publications/#huang2024), [ISSTA 2024](/publications/#he2024unitsyn), [ACL 2023](/publications/#zhao2023)).
- Evaluating AI models for code ([EMNLP 2024](/publications/#xiong2024))
- Testing software automatically ([ICSE 2025](/publications/#rong2025irfuzzer) [CCS 2024](/publications/#lyu2024PromptFuzz), [CCS 2023](/publications/#chen2023Hopper)).

## Trustworthy AI

While machine learning has made great strides, it isn't ready for many scenarios because of concerns over its security, privacy, and robustness. We are making progress towards trustworthy AI, including:

- Attack against safety-aligned LLMs ([NeurIPS 2024](/publications/#li2024llm))
- A benchmark for evaluating transfer-based attacks ([NeurIPS 2023](/publications/#li2023Attack))
- Improving adversarial transferability via intermediate-level perturbation decay ([NeurIPS 2023](/publications/#li2023ILPD)) 
- Use squeeze training for improving adversarial robustness ([ICLR 2023](/publications/#li2023Squeeze))
- Attack Bayesian models to improve transferability of adversarial examples ([ICLR 2023](/publications/#li2023Bayesian))
