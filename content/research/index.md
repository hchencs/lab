+++
title = "Research"
page_template = "page.html"
+++

## AI-driven security

We believe that machine learning has great potential in advancing both defensive and offensive technologies, and we are striving to unleash that potential. Our recent results include:

- Pairing test with tested functions improves test generation of Large Language Models (LLM) ([ISSTA 2024](/publications/#he2024unitsyn)).
- LLMs improve fuzz driver generation ([CCS 2024](/publications/#lyu2024PromptFuzz)).
- Learning-based interpretative fuzzing improves library fuzzing ([CCS 2023](/publications/#chen2023Hopper)).
- Fuzzing improves program understanding and code representation learning of LLMs ([ACL 2023](/publications/#zhao2023)).

## Trustworthy AI

While machine learning has made great strides, it isn't ready for many scenarios because of concerns over its security, privacy, and robustness. We endeavor to make progress towards trustworthy AI. Our recent explorations include:

- A benchmark for evaluating transfer-based attacks ([NeurIPS 2023](/publications/#li2023Attack))
- Improving adversarial transferability via intermediate-level perturbation decay ([NeurIPS 2023](/publications/#li2023ILPD)) 
- Use squeeze training for improving adversarial robustness ([ICLR 2023](/publications/#li2023Squeeze))
- Attack Bayesian models to improve transferability of adversarial examples ([ICLR 2023](/publications/#li2023Bayesian))
