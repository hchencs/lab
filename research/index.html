<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content="noai, noimageai" name=robots><title>JC STEM Lab of Intelligent Cybersecurity - University of Hong Kong</title><meta content="Our lab conducts research in computer and information security and machine learning." name=description><link href=https://sec.hku.hk/research/ rel=canonical><link href=/img/favicons/site.webmanifest rel=manifest><link href=/img/favicons/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/img/favicons/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/img/favicons/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><meta content="Zola v.0.18" name=generator><link href=https://sec.hku.hk/css/style.css rel=stylesheet><body><a class=visually-hidden href=#skip>Skip to main content</a><header><div class="header-container, row"><div class=logo><a href=/><img alt="JC STEM Lab of Intelligent Cybersecurity - University of Hong Kong" src=/img/logo.svg width=300></a></div><div><nav class=menu><ul class=nav><li class=nav-item><a href=" https://sec.hku.hk/research/
                            " aria-current=page>Research </a><li class=nav-item><a href=" https://sec.hku.hk/publications/
                            ">Publications </a><li class=nav-item><a href=" https://sec.hku.hk/people/
                            ">People </a><li class=nav-item><a href=" https://sec.hku.hk/resources/
                            ">Resources </a></ul></nav></div></div></header><main class=section id=skip><h2 id=ai-driven-security-and-software-engineering>AI-driven security and software engineering</h2><p>Modern society depends on critical infrastructure run by large, complex software, but software is expensive to develop and difficult to verify. AI will change how we build, verify, and maintain software. Towards this goal, we are making progress on:<ul><li>Teaching AI models to understand and generate code (<a href=/publications/#huang2024>EMNLP 2024</a>, <a href=/publications/#he2024unitsyn>ISSTA 2024</a>, <a href=/publications/#zhao2023>ACL 2023</a>).<li>Evaluating AI models for code (<a href=/publications/#xiong2024>EMNLP 2024</a>)<li>Testing software automatically (<a href=/publications/#rong2025irfuzzer>ICSE 2025</a> <a href=/publications/#lyu2024PromptFuzz>CCS 2024</a>, <a href=/publications/#chen2023Hopper>CCS 2023</a>).</ul><h2 id=trustworthy-ai>Trustworthy AI</h2><p>While machine learning has made great strides, it isnâ€™t ready for many scenarios because of concerns over its security, privacy, and robustness. We are making progress towards trustworthy AI, including:<ul><li>Attack against safety-aligned LLMs (<a href=/publications/#li2024llm>NeurIPS 2024</a>)<li>A benchmark for evaluating transfer-based attacks (<a href=/publications/#li2023Attack>NeurIPS 2023</a>)<li>Improving adversarial transferability via intermediate-level perturbation decay (<a href=/publications/#li2023ILPD>NeurIPS 2023</a>)<li>Use squeeze training for improving adversarial robustness (<a href=/publications/#li2023Squeeze>ICLR 2023</a>)<li>Attack Bayesian models to improve transferability of adversarial examples (<a href=/publications/#li2023Bayesian>ICLR 2023</a>)</ul></main><footer><p class=powered-by>Updated: 2024-12-11. Powered by <a href=https://www.getzola.org/>Zola</a>. </footer>